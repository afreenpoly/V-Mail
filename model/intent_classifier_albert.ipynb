{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, AdamW, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"vineetsharma/customer-support-intent-albert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"dataset_csv.csv\", split='train')\n",
    "full_dataset = dataset.class_encode_column(\"label\").train_test_split(test_size=0.2, stratify_by_column=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 2766.03 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1664.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"prompt\"], truncation=True, return_tensors=\"pt\", padding=True)\n",
    "tokenized_datasets = full_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"prompt\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aadil Sayad\\Documents\\Local Disk D (real)\\Projects\\V_Mail\\.venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 124/125 [00:10<00:00, 12.59it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this -> delete_mail\n"
     ]
    }
   ],
   "source": [
    "intent_map = {0: \"compose_mail\", 1: \"delete_mail\", 2: \"read_next\", 3: \"star_mail\", 4: \"view_inbox\"}\n",
    "input_text = [\"I want to write an email. Can you help me?\", \"Please get rid of this mail.\", \"Read the following mail.\", \"Can you please star this email?\", \"I'd like to take a look at my inbox.\"]\n",
    "input_one = \" this\"\n",
    "encoded_input = tokenizer(input_one, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "    logits = output.logits\n",
    "    predicted_intent = logits.argmax(-1).item()\n",
    "    print(f\"{input_one} -> {intent_map[predicted_intent]}\")\n",
    "# for item in input_text:\n",
    "#     encoded_input = tokenizer(item, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     with torch.no_grad():\n",
    "#         output = model(**encoded_input)\n",
    "#         logits = output.logits\n",
    "#         predicted_intent = logits.argmax(-1).item()\n",
    "#     print(f\"{item} -> {intent_map[predicted_intent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag up this message with a star. -> star_mail\n",
      "Can you get to the next message? -> read_next\n",
      "I have to write an email. -> compose_mail\n",
      "Can we bookmark the email? -> star_mail\n",
      "Can you access the next message? -> read_next\n",
      "Let's get to the next message. -> read_next\n",
      "I need to flag up the message with a star. -> star_mail\n",
      "Remove this email. -> delete_mail\n",
      "Go ahead and star the email. -> star_mail\n",
      "I’d like to compose an email. -> compose_mail\n",
      "I need to discard the message. -> delete_mail\n",
      "Can you advance forward towards reading out loud, my following mail? -> read_next\n",
      "Go through my mail -> view_inbox\n",
      "Can you help me send a mail? -> compose_mail\n",
      "Can you annihilate the message from my inbox. -> delete_mail\n",
      "Can you show the email messages? -> view_inbox\n",
      "I'd like to wipe out the message. -> delete_mail\n",
      "Move forward towards reading out loud, my following mail. -> read_next\n",
      "Can you show me what’s in my mailbox? -> view_inbox\n",
      "Label this message as starred. -> star_mail\n",
      "I need to check my inbox -> view_inbox\n",
      "Can we label the message as starred. -> star_mail\n",
      "Write an email. -> compose_mail\n",
      "Can you trash this message? -> delete_mail\n",
      "Bring up what’s in my mailbox -> view_inbox\n",
      "Can we open what’s in my mailbox -> view_inbox\n",
      "I need to open my inbox -> view_inbox\n",
      "Begin purging the message from my inbox. -> delete_mail\n",
      "I want you to proceed towards reading out loud, my following mail. -> read_next\n",
      "Open up a new email. -> compose_mail\n",
      "I want to open the email inbox -> view_inbox\n",
      "Read the next email. -> read_next\n",
      "Let's star this email. -> star_mail\n",
      "Can we mark this message with a star. -> star_mail\n",
      "I should compose an email. -> compose_mail\n",
      "Draft a mail. -> compose_mail\n",
      "Let's read the next email. -> read_next\n",
      "I'd like to reach out for the next message. -> read_next\n",
      "I'd like to star this email now. -> star_mail\n",
      "Let's delete this email. -> delete_mail\n",
      "Unveil what’s in my mailbox -> view_inbox\n",
      "Can you delete this email? -> delete_mail\n",
      "Highlight this message with a star. -> star_mail\n",
      "I want to send an email. -> compose_mail\n",
      "I need to remove this email. -> delete_mail\n",
      "Start trashing the message. -> delete_mail\n",
      "Can you help me write an email? -> compose_mail\n",
      "Can you open my inbox? -> view_inbox\n",
      "I wish to begin an email. -> compose_mail\n",
      "Can you move forward towards reading out loud, my following mail. -> read_next\n"
     ]
    }
   ],
   "source": [
    "intent_map = {0: \"compose_mail\", 1: \"delete_mail\", 2: \"read_next\", 3: \"star_mail\", 4: \"view_inbox\"}\n",
    "input_text = [item['prompt'] for item in full_dataset['test']]\n",
    "for item in input_text:\n",
    "    encoded_input = tokenizer(item, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "        logits = output.logits\n",
    "        predicted_intent = logits.argmax(-1).item()\n",
    "    print(f\"{item} -> {intent_map[predicted_intent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_tokenizer\\\\tokenizer_config.json',\n",
       " 'trained_tokenizer\\\\special_tokens_map.json',\n",
       " 'trained_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"trained_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = AutoModelForSequenceClassification.from_pretrained(\"trained_model\")\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "new_model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained(\"trained_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag up this message with a star. -> star_mail\n",
      "Can you get to the next message? -> read_next\n",
      "I have to write an email. -> compose_mail\n",
      "Can we bookmark the email? -> star_mail\n",
      "Can you access the next message? -> read_next\n",
      "Let's get to the next message. -> read_next\n",
      "I need to flag up the message with a star. -> star_mail\n",
      "Remove this email. -> delete_mail\n",
      "Go ahead and star the email. -> star_mail\n",
      "I’d like to compose an email. -> compose_mail\n",
      "I need to discard the message. -> delete_mail\n",
      "Can you advance forward towards reading out loud, my following mail? -> read_next\n",
      "Go through my mail -> view_inbox\n",
      "Can you help me send a mail? -> compose_mail\n",
      "Can you annihilate the message from my inbox. -> delete_mail\n",
      "Can you show the email messages? -> view_inbox\n",
      "I'd like to wipe out the message. -> delete_mail\n",
      "Move forward towards reading out loud, my following mail. -> read_next\n",
      "Can you show me what’s in my mailbox? -> view_inbox\n",
      "Label this message as starred. -> star_mail\n",
      "I need to check my inbox -> view_inbox\n",
      "Can we label the message as starred. -> star_mail\n",
      "Write an email. -> compose_mail\n",
      "Can you trash this message? -> delete_mail\n",
      "Bring up what’s in my mailbox -> view_inbox\n",
      "Can we open what’s in my mailbox -> view_inbox\n",
      "I need to open my inbox -> view_inbox\n",
      "Begin purging the message from my inbox. -> delete_mail\n",
      "I want you to proceed towards reading out loud, my following mail. -> read_next\n",
      "Open up a new email. -> compose_mail\n",
      "I want to open the email inbox -> view_inbox\n",
      "Read the next email. -> read_next\n",
      "Let's star this email. -> star_mail\n",
      "Can we mark this message with a star. -> star_mail\n",
      "I should compose an email. -> compose_mail\n",
      "Draft a mail. -> compose_mail\n",
      "Let's read the next email. -> read_next\n",
      "I'd like to reach out for the next message. -> read_next\n",
      "I'd like to star this email now. -> star_mail\n",
      "Let's delete this email. -> delete_mail\n",
      "Unveil what’s in my mailbox -> view_inbox\n",
      "Can you delete this email? -> delete_mail\n",
      "Highlight this message with a star. -> star_mail\n",
      "I want to send an email. -> compose_mail\n",
      "I need to remove this email. -> delete_mail\n",
      "Start trashing the message. -> delete_mail\n",
      "Can you help me write an email? -> compose_mail\n",
      "Can you open my inbox? -> view_inbox\n",
      "I wish to begin an email. -> compose_mail\n",
      "Can you move forward towards reading out loud, my following mail. -> read_next\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:29<00:00, 12.59it/s]"
     ]
    }
   ],
   "source": [
    "intent_map = {0: \"compose_mail\", 1: \"delete_mail\", 2: \"read_next\", 3: \"star_mail\", 4: \"view_inbox\"}\n",
    "input_text = [item['prompt'] for item in full_dataset['test']]\n",
    "for item in input_text:\n",
    "    encoded_input = new_tokenizer(item, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = new_model(**encoded_input)\n",
    "        logits = output.logits\n",
    "        predicted_intent = logits.argmax(-1).item()\n",
    "    print(f\"{item} -> {intent_map[predicted_intent]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
